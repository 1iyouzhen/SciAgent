"""
SciAgent - å¯éªŒè¯çš„ç§‘å­¦æ–‡çŒ®é—®ç­”ç³»ç»Ÿ

æ ¸å¿ƒç‰¹æ€§ï¼š
- Verifiableï¼ˆå¯éªŒè¯ï¼‰ï¼šå¼ºåˆ¶å¼•ç”¨ + ç½®ä¿¡åº¦ + è‡ªæˆ‘æ ¡éªŒ
- Retrieval-Augmentedï¼ˆæ£€ç´¢å¢å¼ºï¼‰ï¼šQwen3-Embedding + å¤šæ¨¡æ€æ··åˆæ£€ç´¢
- Agent-Collaborativeï¼ˆAgentåä½œï¼‰ï¼šsmolagentså¤šæ™ºèƒ½ä½“ååŒ
- Iterativeï¼šReviewer â†’ Retriever è¿­ä»£ä¼˜åŒ–

å·¥ä½œæµç¨‹ï¼š
ç”¨æˆ·é—®é¢˜ â†’ Planner â†’ [Retriever, Caption, Reasoner] â†’ Reviewer â†’ æœ€ç»ˆè¾“å‡º
"""

import os
from pathlib import Path

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)
import yaml
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from pathlib import Path


@dataclass
class PipelineOutput:
    """æµæ°´çº¿è¾“å‡ºç»“æœ"""
    question: str
    sub_tasks: List[Dict[str, Any]]
    final_answer: str
    citations: List[Dict[str, Any]]
    confidence: float
    iterate_count: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "question": self.question,
            "sub_tasks": self.sub_tasks,
            "final_answer": self.final_answer,
            "citations": self.citations,
            "confidence": self.confidence,
            "iterate_count": self.iterate_count,
            "metadata": self.metadata
        }
    
    def format_output(self) -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        lines = [
            "=" * 60,
            "ğŸ“‹ é—®é¢˜",
            "-" * 60,
            self.question,
            "",
            "=" * 60,
            "ğŸ“ ç­”æ¡ˆ",
            "-" * 60,
            self.final_answer,
            "",
            "=" * 60,
            f"ğŸ“Š ç½®ä¿¡åº¦: {self.confidence:.2%}",
            f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {self.iterate_count}",
            "",
            "=" * 60,
            "ğŸ“š å¼•ç”¨",
            "-" * 60,
        ]
        
        for i, cite in enumerate(self.citations[:10]):
            source = cite.get("source", "æœªçŸ¥æ¥æº")
            page = cite.get("page", "?")
            quote = cite.get("quote", "")[:100]
            lines.append(f"[{i+1}] {source} (p.{page})")
            if quote:
                lines.append(f"    \"{quote}...\"")
        
        lines.append("=" * 60)
        return "\n".join(lines)


def _load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    defaults = {
        "data": {
            "pdf_dir": "data/pdfs",
            "processed_dir": "data/processed",
            "index_path": "data/index.faiss"
        },
        "models": {
            "embed_model": "Qwen/Qwen3-Embedding-0.6B",
            "vl_model": "glm-4v-flash",
            "reasoner_model": "glm-4-flash",
            "api_provider": "dashscope",
            "api_base": None,  # è®© LLMClient æ ¹æ® api_provider è‡ªåŠ¨é€‰æ‹©
            "api_key": ""
        },
        "pipeline": {
            "top_k": 10,
            "max_iterations": 3,
            "confidence_threshold": 0.6,
            "chunk_size": 1024,
            "chunk_overlap": 50
        },
        "agents": {
            "planner": {"max_subtasks": 5},
            "retriever": {"rerank": True, "hybrid": True},
            "reviewer": {"use_llm_judge": True}
        }
    }
    
    try:
        config_path = Path(__file__).parent / "config.yaml"
        if config_path.exists():
            with open(config_path, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
                # æ·±åº¦åˆå¹¶é…ç½®
                for k, v in defaults.items():
                    if k not in cfg:
                        cfg[k] = v
                    elif isinstance(v, dict):
                        for kk, vv in v.items():
                            if kk not in cfg[k]:
                                cfg[k][kk] = vv
                return cfg
    except Exception as e:
        print(f"[Warning] é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    return defaults


def _ensure_dirs(paths: List[str]) -> None:
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    for d in paths:
        os.makedirs(d, exist_ok=True)


def _resolve_path(path: str, base_dir: Path) -> Path:
    """è§£æè·¯å¾„"""
    p = Path(path)
    if p.is_absolute():
        return p
    return base_dir / path


class SciAgentPipeline:
    """
    ç§‘å­¦æ–‡çŒ®é—®ç­”æµæ°´çº¿
    
    å®ç°å¤šæ™ºèƒ½ä½“åä½œçš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or _load_config()
        self.base_dir = Path(__file__).parent
        
        # åˆå§‹åŒ–è·¯å¾„
        self.pdf_dir = _resolve_path(self.config["data"]["pdf_dir"], self.base_dir)
        self.processed_dir = _resolve_path(self.config["data"]["processed_dir"], self.base_dir)
        self.index_path = _resolve_path(self.config["data"]["index_path"], self.base_dir)
        
        _ensure_dirs([str(self.pdf_dir), str(self.processed_dir)])
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶
        self._llm_client = None
        self._vl_client = None
        self._vector_db = None
        self._agents = {}
    
    @property
    def llm_client(self):
        """LLMå®¢æˆ·ç«¯ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._llm_client is None:
            from .tools.llm_client import LLMClient
            self._llm_client = LLMClient(
                model=self.config["models"]["reasoner_model"],
                api_provider=self.config["models"].get("api_provider", "dashscope"),
                api_base=self.config["models"].get("api_base"),
                api_key=self.config["models"].get("api_key")
            )
        return self._llm_client
    
    @property
    def vl_client(self):
        """VLå®¢æˆ·ç«¯ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._vl_client is None:
            from .tools.llm_client import VLClient
            self._vl_client = VLClient(
                model=self.config["models"]["vl_model"],
                api_provider=self.config["models"].get("api_provider", "dashscope"),
                api_base=self.config["models"].get("api_base"),
                api_key=self.config["models"].get("api_key")
            )
        return self._vl_client
    
    @property
    def vector_db(self):
        """å‘é‡æ•°æ®åº“ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._vector_db is None:
            from .tools.vector_db import VectorDB
            self._vector_db = VectorDB(
                embed_model=self.config["models"]["embed_model"],
                index_path=str(self.index_path)
            )
            # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
            loaded = self._vector_db.load_index()
            if not loaded:
                print(f"[Warning] ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆè¿è¡Œ build_index æ„å»ºç´¢å¼•")
            else:
                print(f"[Info] ç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {len(self._vector_db.docs)}")
        return self._vector_db
    
    def _get_agent(self, name: str):
        """è·å–Agentï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if name not in self._agents:
            from .agents import (
                PlannerAgent, RetrieverAgent, CaptionAgent,
                MultiLLMReasonerAgent, ReviewerAgent, IterativeReviewer
            )
            
            agent_config = self.config.get("agents", {}).get(name, {})
            
            if name == "planner":
                self._agents[name] = PlannerAgent(
                    config=agent_config,
                    llm_client=self.llm_client
                )
            elif name == "retriever":
                self._agents[name] = RetrieverAgent(
                    db=self.vector_db,
                    config={**agent_config, "top_k": self.config["pipeline"]["top_k"]},
                    llm_client=self.llm_client
                )
            elif name == "caption":
                self._agents[name] = CaptionAgent(
                    config=agent_config,
                    vl_client=self.vl_client
                )
            elif name == "reasoner":
                # ä½¿ç”¨ MultiLLMReasonerAgent æ›¿ä»£åŸ ReasonerAgent
                # ä»é…ç½®ä¸­è¯»å–å¤šLLMè®¾ç½®
                reasoning_config = self.config.get("agents", {}).get("reasoning", {})
                
                # åˆ›å»ºæ–‡æœ¬æ¨ç†LLM
                text_llm = self.llm_client  # é»˜è®¤ä½¿ç”¨ä¸»LLM
                math_llm = None
                
                # å¦‚æœé…ç½®äº†ä¸“é—¨çš„æ–‡æœ¬æ¨ç†æ¨¡å‹
                if reasoning_config.get("text_reasoner"):
                    text_cfg = reasoning_config["text_reasoner"]
                    from .tools.llm_client import LLMClient
                    text_llm = LLMClient(
                        model=text_cfg.get("model", self.config["models"]["reasoner_model"]),
                        api_provider=text_cfg.get("provider", self.config["models"]["api_provider"])
                    )
                    print(f"[Info] æ–‡æœ¬æ¨ç†æ¨¡å‹: {text_cfg.get('model')} ({text_cfg.get('provider')})")
                
                # å¦‚æœé…ç½®äº†æ•°å­¦æ¨ç†æ¨¡å‹
                if reasoning_config.get("math_reasoner"):
                    math_cfg = reasoning_config["math_reasoner"]
                    from .tools.llm_client import LLMClient
                    math_llm = LLMClient(
                        model=math_cfg.get("model"),
                        api_provider=math_cfg.get("provider", self.config["models"]["api_provider"])
                    )
                    print(f"[Info] æ•°å­¦æ¨ç†æ¨¡å‹: {math_cfg.get('model')} ({math_cfg.get('provider')})")
                
                # æ‰“å°é›†æˆæ¨¡å‹é…ç½®
                if reasoning_config.get("ensemble_models"):
                    print(f"[Info] é›†æˆæ¨ç†æ¨¡å‹é…ç½®:")
                    for em in reasoning_config["ensemble_models"]:
                        print(f"       - {em.get('model')} (æƒé‡: {em.get('weight', 0)})")
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"[Debug] reasoning_config.enable_ensemble: {reasoning_config.get('enable_ensemble')}")
                print(f"[Debug] reasoning_config.strategy: {reasoning_config.get('strategy')}")
                print(f"[Debug] reasoning_config.ensemble_models count: {len(reasoning_config.get('ensemble_models', []))}")
                
                self._agents[name] = MultiLLMReasonerAgent(
                    config={"reasoning": reasoning_config},  # ç›´æ¥ä¼ é€’ reasoning é…ç½®
                    text_llm_client=text_llm,
                    math_llm_client=math_llm,
                    llm_client=text_llm  # å…¼å®¹å‚æ•°å
                )
            elif name == "reviewer":
                self._agents[name] = IterativeReviewer(
                    config={**agent_config, "confidence_threshold": self.config["pipeline"]["confidence_threshold"]},
                    llm_client=self.llm_client,
                    retriever=self._get_agent("retriever")
                )
        
        return self._agents[name]
    
    def build_index(self, pdf_dir: str = None) -> int:
        """
        æ„å»ºæ–‡æ¡£ç´¢å¼•
        Args:
            pdf_dir: PDFç›®å½•è·¯å¾„
            
        Returns:
            ç´¢å¼•çš„æ–‡æ¡£å—æ•°é‡
        """
        from .tools.pdf_parser import PdfParser
        
        pdf_dir = Path(pdf_dir) if pdf_dir else self.pdf_dir
        parser = PdfParser(output_dir=str(self.processed_dir))
        
        all_chunks = []
        chunk_size = self.config["pipeline"]["chunk_size"]
        chunk_overlap = self.config["pipeline"]["chunk_overlap"]
        
        # éå†PDFç›®å½•
        for file_path in pdf_dir.iterdir():
            if not file_path.is_file():
                continue
            
            suffix = file_path.suffix.lower()
            if suffix not in [".pdf", ".txt"]:
                continue
            
            print(f"[Info] è§£ææ–‡æ¡£: {file_path.name}")
            
            try:
                doc = parser.parse(str(file_path))
                chunks = parser.to_chunks(doc, chunk_size=chunk_size, overlap=chunk_overlap)
                all_chunks.extend(chunks)
                print(f"  -> ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
            except Exception as e:
                print(f"  -> è§£æå¤±è´¥: {e}")
        
        # æ„å»ºç´¢å¼•
        if all_chunks:
            print(f"[Info] æ„å»ºå‘é‡ç´¢å¼•ï¼Œå…± {len(all_chunks)} ä¸ªæ–‡æ¡£å—...")
            self.vector_db.index_documents(all_chunks)
            print("[Info] ç´¢å¼•æ„å»ºå®Œæˆ")
        
        return len(all_chunks)
    
    def run(self, question: str) -> PipelineOutput:
        """
        è¿è¡Œé—®ç­”æµæ°´çº¿
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            PipelineOutputå¯¹è±¡
        """
        from .agents.base import AgentContext
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        context = AgentContext(
            question=question,
            max_iterations=self.config["pipeline"]["max_iterations"]
        )
        
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ é—®é¢˜: {question}")
        print(f"{'='*60}\n")
        
        # Step 1: Planner - ä»»åŠ¡åˆ†è§£
        print("[Step 1] Planner: ä»»åŠ¡åˆ†è§£...")
        planner = self._get_agent("planner")
        planner_result = planner.run(context)
        if planner_result.success:
            context.sub_tasks = planner_result.data.get("sub_tasks", [])
            print(f"  -> åˆ†è§£ä¸º {len(context.sub_tasks)} ä¸ªå­ä»»åŠ¡")
            for task in context.sub_tasks:
                print(f"     - {task.get('task', '')[:50]}")
        
        # Step 2: Retriever - æ£€ç´¢
        print("\n[Step 2] Retriever: å¤šæ¨¡æ€æ£€ç´¢...")
        retriever = self._get_agent("retriever")
        retriever_result = retriever.run(context)
        if retriever_result.success:
            context.evidences = retriever_result.data.get("evidences", [])
            print(f"  -> æ£€ç´¢åˆ° {len(context.evidences)} æ¡è¯æ®")
        
        # Step 3: Caption - å›¾åƒç†è§£
        print("\n[Step 3] Caption: å›¾åƒç†è§£...")
        # å…ˆç»Ÿè®¡è¯æ®ä¸­çš„å›¾åƒæ•°é‡
        image_evidences = [ev for ev in context.evidences if ev.get("chunk_type") == "image"]
        print(f"[Info] VLMé…ç½®: provider={self.config['models'].get('api_provider')}, model={self.config['models'].get('vl_model')}")
        
        if len(image_evidences) == 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºPDFè§£æå™¨ä¸æ”¯æŒå›¾åƒ
            has_pypdf_docs = any(
                ev.get("metadata", {}).get("image_support") == False 
                for ev in context.evidences
            )
            if has_pypdf_docs or len(context.evidences) > 0:
                print(f"[Info] è¯æ®ä¸­æ²¡æœ‰å›¾åƒç±»å‹ã€‚å¯èƒ½åŸå› ï¼š")
                print(f"       1. PDFä½¿ç”¨PyPDF2è§£æï¼Œä¸æ”¯æŒå›¾åƒæå–")
                print(f"       2. å¦‚éœ€å›¾åƒç†è§£ï¼Œè¯·å®‰è£…MinerU: pip install magic-pdf")
                print(f"       3. å®‰è£…åéœ€é‡æ–°æ„å»ºç´¢å¼•: python -m sci_agent.main --build-index")
        else:
            print(f"[Info] è¯æ®ä¸­å›¾åƒç±»å‹æ•°é‡: {len(image_evidences)}")
        
        caption_agent = self._get_agent("caption")
        caption_result = caption_agent.run(context)
        if caption_result.success:
            context.captions = caption_result.data.get("captions", [])
            print(f"  -> å¤„ç†äº† {len(context.captions)} å¼ å›¾åƒ")
        
        # Step 4: Reasoner - æ¨ç†ç”Ÿæˆï¼ˆä½¿ç”¨ MultiLLMReasonerAgentï¼‰
        print("\n[Step 4] Reasoner: æ¨ç†ç”Ÿæˆ...")
        reasoner = self._get_agent("reasoner")
        reasoner_result = reasoner.run(context)
        if reasoner_result.success:
            data = reasoner_result.data
            context.draft_answer = data.get("answer", "")
            # ä» reasoning_trace æˆ–ç›´æ¥ä» data è·å– citations
            if "reasoning_trace" in data and hasattr(data["reasoning_trace"], "document_sources"):
                # è½¬æ¢ document_sources ä¸º citations æ ¼å¼
                trace = data["reasoning_trace"]
                context.citations = []
                for i, src in enumerate(trace.document_sources):
                    context.citations.append({
                        "id": i + 1,
                        "source": src.get("source", ""),
                        "doc_id": src.get("doc_id", ""),
                        "page": src.get("pages", [0])[0] if src.get("pages") else 0
                    })
            else:
                context.citations = data.get("citations", [])
            print(f"  -> ç”Ÿæˆç­”æ¡ˆï¼ŒåŒ…å« {len(context.citations)} æ¡å¼•ç”¨")
        
        # Step 5: Reviewer - è‡ªæˆ‘æ ¡éªŒï¼ˆå¸¦è¿­ä»£ï¼‰
        print("\n[Step 5] Reviewer: è‡ªæˆ‘æ ¡éªŒ...")
        reviewer = self._get_agent("reviewer")
        
        # ä½¿ç”¨è¿­ä»£å®¡æ ¸
        if hasattr(reviewer, 'run_with_iteration'):
            context = reviewer.run_with_iteration(context)
        else:
            reviewer_result = reviewer.run(context)
            if reviewer_result.success:
                context.confidence = reviewer_result.data.get("confidence", 0.0)
                context.draft_answer = reviewer_result.data.get("final_answer", context.draft_answer)
        
        print(f"  -> ç½®ä¿¡åº¦: {context.confidence:.2%}")
        print(f"  -> è¿­ä»£æ¬¡æ•°: {context.iteration}")
        
        # æ„å»ºè¾“å‡º
        output = PipelineOutput(
            question=question,
            sub_tasks=context.sub_tasks,
            final_answer=context.draft_answer,
            citations=context.citations,
            confidence=context.confidence,
            iterate_count=context.iteration,
            metadata={
                "evidence_count": len(context.evidences),
                "caption_count": len(context.captions)
            }
        )
        
        return output


# å…¼å®¹æ—§æ¥å£
def build_index(pdf_dir: str) -> List[Dict[str, Any]]:
    """æ„å»ºç´¢å¼•ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    pipeline = SciAgentPipeline()
    count = pipeline.build_index(pdf_dir)
    return [{"count": count}]


def run_pipeline(question: str) -> Dict[str, Any]:
    """è¿è¡Œæµæ°´çº¿ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    pipeline = SciAgentPipeline()
    output = pipeline.run(question)
    return output.to_dict()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SciAgent - å¯éªŒè¯çš„ç§‘å­¦æ–‡çŒ®é—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--question", "-q", type=str, help="é—®é¢˜")
    parser.add_argument("--build-index", "-b", action="store_true", help="æ„å»ºç´¢å¼•")
    parser.add_argument("--pdf-dir", type=str, help="PDFç›®å½•")
    parser.add_argument("--interactive", "-i", action="store_true", help="äº¤äº’æ¨¡å¼")
    
    args = parser.parse_args()
    
    pipeline = SciAgentPipeline()
    
    # æ„å»ºç´¢å¼•
    if args.build_index:
        pdf_dir = args.pdf_dir or str(pipeline.pdf_dir)
        print(f"[Info] æ„å»ºç´¢å¼•: {pdf_dir}")
        count = pipeline.build_index(pdf_dir)
        print(f"[Info] ç´¢å¼•å®Œæˆï¼Œå…± {count} ä¸ªæ–‡æ¡£å—")
        return
    
    # äº¤äº’æ¨¡å¼
    if args.interactive:
        print("=" * 60)
        print("SciAgent - å¯éªŒè¯çš„ç§‘å­¦æ–‡çŒ®é—®ç­”ç³»ç»Ÿ")
        print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("=" * 60)
        
        while True:
            try:
                question = input("\nğŸ“‹ è¯·è¾“å…¥é—®é¢˜: ").strip()
                if question.lower() in ["quit", "exit", "q"]:
                    print("å†è§ï¼")
                    break
                if not question:
                    continue
                
                output = pipeline.run(question)
                print("\n" + output.format_output())
            except KeyboardInterrupt:
                print("\nå†è§ï¼")
                break
            except Exception as e:
                print(f"[Error] {e}")
        return
    
    # å•æ¬¡æŸ¥è¯¢
    question = args.question or os.environ.get("SCI_QUESTION", "è¯·æ€»ç»“æ–‡çŒ®çš„å…³é”®ç»“è®ºå¹¶ç»™å‡ºå¼•ç”¨ã€‚")
    output = pipeline.run(question)
    print("\n" + output.format_output())


if __name__ == "__main__":
    main()
